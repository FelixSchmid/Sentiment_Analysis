{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "2_Sentiment_Analysis_IMBD_LSTM_fastText_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FelixSchmid/Sentiment_Analysis/blob/master/2_Sentiment_Analysis_IMBD_LSTM_fastText_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LE48lNm26XwJ"
      },
      "source": [
        "# Importing data and libraries (notebook 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gEwjTzd8rqjR",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u65tZZ6BrDm-",
        "colab": {}
      },
      "source": [
        "# Mounting colab with my drive to import GloVe vectors\n",
        "# and safe models\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5FcA2wo-Z_EK",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"; "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eYzTJFQXgXuF",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "! pip install ktrain\n",
        "! pip3 install git+https://github.com/amaiya/eli5@tfkeras_0_10_1\n",
        "import ktrain\n",
        "from ktrain import text\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from tensorflow.keras.layers import Dropout, Bidirectional, GlobalAveragePooling1D\n",
        "from tensorflow.keras import backend as K \n",
        "from tensorflow.keras.initializers import Constant\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "#pip install keras-self-attention\n",
        "os.environ['TF_KERAS'] = '1'\n",
        "from keras_self_attention import SeqSelfAttention, SeqWeightedAttention\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jTlw-i9kMuZj"
      },
      "source": [
        "# 2. LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WMmdeFlmHH7L"
      },
      "source": [
        "## 2.0 Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kq4aAvujHH-p"
      },
      "source": [
        "So far, we received a decent baseline performance with the TFIDF model. However, because TFIDF is based on BoW, this approach can not properly process contextual information (except locally with the help of ngrams).\n",
        "\n",
        "In this section, we will use an LSTM which is well-suited for time series. A sentence or a text can be interpreted as a time series. The simplest approach would be to use a single LSTM layer that reads in the sequence from left to right. Each word in the sequence can be interpreted as one time step. The key part of the LSTM is the cell state. This is so to say the memory of the LSTM. \n",
        "\n",
        "In our context, that is where information from previous words can be stored. There are three gates (input gate, output gate, forget gate) which regulate what information from which word will be stored in the cell state and what information from the cell state will be given to the next time step. The behaviour of the gates will be learned during training. In other words, the model will learn which information to store, output or forget during training. The LSTM were invented by [Hochreiter & Schmidhuber (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf).\n",
        "\n",
        "<img src=https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png width=\"500\">\n",
        "\n",
        "\n",
        "[Illustration from colah's blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "\n",
        "\n",
        "In theory, a word could be 'remembered' in the cell state for an infinite number of time steps. In our context that means that the first word of a sequence could be stored until the end of the sequence. So, contextual information are not only processed locally but ideally over the whole sequence.\n",
        "\n",
        "However, there is a limitation. An LSTM can only carry on information in one direction. That is not good, because not only prior words are relevant for understanding the context of a word.\n",
        "\n",
        "Therefore, we will use a bidirectional LSTMs. In this architecture we have two parallel LSTMs. In one, the sequence is inputted inverse and in the other, the normal way. In the next step, the output of both LSTMs is concatenated. This way the resulting vector can contain information from the beginning and the end of the sequence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l5PwX3-603XL"
      },
      "source": [
        "## 2.1 Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dE1XCvLaKS3W"
      },
      "source": [
        "In my first notebook for sentiment analysis with TFIDF, I wrote the importing and preprocessing manually using spaCy and gensim. However, ktrain already has a build in function for efficiently importing and preprocessing text data. The function even creates ngrams (if wanted) on the fly. It also prints out a handy overview of the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dlxqwdmDa0KI",
        "outputId": "6354c1a6-192d-4b51-89cc-a49b7230afa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Importing data\n",
        "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_folder('aclImdb', \n",
        "                                                                         max_features=20000,\n",
        "                                                                         maxlen=400,\n",
        "                                                                         preprocess_mode='standard',\n",
        "                                                                         classes=['pos', 'neg'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "detected encoding: utf-8\n",
            "language: en\n",
            "Word Counts: 88582\n",
            "Nrows: 25000\n",
            "25000 train sequences\n",
            "Average train sequence length: 231\n",
            "x_train shape: (25000,400)\n",
            "y_train shape: (25000,2)\n",
            "25000 test sequences\n",
            "Average test sequence length: 224\n",
            "x_test shape: (25000,400)\n",
            "y_test shape: (25000,2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zt8s2X-lU52h",
        "colab": {}
      },
      "source": [
        "# Here, we split the test data to receive a holdout set. We do not want to \n",
        "# leak information by tuning hyperparameters\n",
        "x_test, x_holdout, y_test, y_holdout = train_test_split(x_test, \n",
        "                                                        y_test, \n",
        "                                                        test_size=0.5, \n",
        "                                                        random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XImhsr9h2q_r"
      },
      "source": [
        "**Data after preprocessing**\n",
        "\n",
        "The maximum lenght after preprocessing is set to 400. When a senquence (movie review) is shorter, it is padded with 0. Each word is represented by an integer (0 to 19999). The maximum amount of features is set to 20000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sf4Fczq8A_zx",
        "outputId": "d9d19f2b-4f77-4a11-c8ea-4ed82ce97a8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "print('feature vector of first document in train set:')\n",
        "print(x_train[0])\n",
        "print('target vector of first document in train set:')\n",
        "print(y_train[0])\n",
        "print('highest word index in train set: ' + str(np.amax(x_train)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "feature vector of first document in train set:\n",
            "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0  1453   248   829    22     5   101    57   792   101\n",
            "   135   104  1009   182   346    59    78    48    33   119  3513  5975\n",
            "  1714  2847 16115    65  8169     9  2338    48   212    27   721     3\n",
            "  1158  5399     4   109    15   104  1706    34    25   869     5    36\n",
            "  1138     8   657     5  6702    65   202  5975   179  2847  5844  3249\n",
            "     7     7     9     6    21     3   400    17    18   345    48   275\n",
            "    55     1  1720     2   153    66     9     6     3  1730  2211     8\n",
            "  1299     4  4114     1  4201     2  1724     4     1   104   182  1714\n",
            "  7476     9     6   125    71     8  1299     4   109     3    19    12\n",
            "   211   463   260  2387     9     6     3   227   227   125    19    71\n",
            "   217   229    22    23  1326     5    64     7     7  3036    18  1199\n",
            "    16     3   391  4348]\n",
            "target vector of first document in train set:\n",
            "[0. 1.]\n",
            "highest word index in train set: 19999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dUM6iSisDi--"
      },
      "source": [
        "The embedding layer will assign each word to a vector with a dimension of 100. During the model training, the embedding vector of each word will also be trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "058AJoEs060r"
      },
      "source": [
        "## 2.2 Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T94uAq5zX-Wp",
        "outputId": "e923b9a0-758e-4c12-c1b7-5742d31ef992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "def load_model(input_shape):\n",
        "    model = Sequential()\n",
        "    # add 1 for padding token\n",
        "    model.add(Embedding(19999+1, 100, mask_zero=True, \n",
        "                        input_length=input_shape[0]))\n",
        "    model.add(SpatialDropout1D(0.4))\n",
        "    model.add(Bidirectional(LSTM(100, dropout=0.4, \n",
        "                                 return_sequences=True)))\n",
        "    model.add(Bidirectional(LSTM(100, dropout=0.4, \n",
        "                                 return_sequences=True)))\n",
        "    model.add(SeqWeightedAttention())\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', \n",
        "                  optimizer='adam', \n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "K.clear_session()\n",
        "input_shape = ((400),)\n",
        "lstm_model = load_model(input_shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 400, 100)          2000000   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d (SpatialDr (None, 400, 100)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 400, 200)          160800    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 400, 200)          240800    \n",
            "_________________________________________________________________\n",
            "seq_weighted_attention (SeqW (None, 200)               201       \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 402       \n",
            "=================================================================\n",
            "Total params: 2,402,203\n",
            "Trainable params: 2,402,203\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xagj13HXoFnw",
        "colab": {}
      },
      "source": [
        "lstm_learner = ktrain.get_learner(lstm_model, \n",
        "                                  train_data=(x_train, y_train), \n",
        "                                  val_data=(x_test, y_test), \n",
        "                                  batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N5yLXPK-zQZg",
        "colab": {}
      },
      "source": [
        "#find a good learning rate\n",
        "#lstm_learner.lr_find()\n",
        "#lstm_learner.lr_plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s0wqWHTSzQZp",
        "outputId": "6b553dc9-82b5-4a04-de5a-d99bc952ce71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "lstm_learner.autofit(0.01, \n",
        "                     5, \n",
        "                     early_stopping=1, \n",
        "                     checkpoint_folder='/content/drive/My Drive/models/lstm')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "begin training using triangular learning rate policy with max lr of 0.01...\n",
            "Train on 25000 samples, validate on 12500 samples\n",
            "Epoch 1/2\n",
            "25000/25000 [==============================] - 1731s 69ms/sample - loss: 0.3554 - acc: 0.8395 - val_loss: 0.2704 - val_acc: 0.8922\n",
            "Epoch 2/2\n",
            "24960/25000 [============================>.] - ETA: 2s - loss: 0.1903 - acc: 0.9274Restoring model weights from the end of the best epoch.\n",
            "25000/25000 [==============================] - 1724s 69ms/sample - loss: 0.1904 - acc: 0.9274 - val_loss: 0.2842 - val_acc: 0.8906\n",
            "Epoch 00002: early stopping\n",
            "Weights from best epoch have been loaded into model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5037364048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HNoEY9_Ke-c-",
        "colab": {}
      },
      "source": [
        "lstm_learner.save_model(\n",
        "    '/content/drive/My Drive/models/lstm/lstm_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EQp5oOmvHbHW"
      },
      "source": [
        "## 2.3 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6pVJnRPL1nZC",
        "colab": {}
      },
      "source": [
        "# I used a custom layer for attention. Unfortunately, I can not load my model\n",
        "# because of that. Luckily, I still can recreate my model and than load the\n",
        "# weights of my best epoch.\n",
        "lstm_learner.model.load_weights('/content/drive/My Drive/models/lstm/weights-01.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TGyEzzwl8xPd"
      },
      "source": [
        "**Analysing the top losses**\n",
        "\n",
        "ktrain has some very nice functions to evaluate your model. In the following we print out the 5 movie reviews with the highest loss. This helps us to understand for which cases our model does perform poorly. \n",
        "If the model fails for reviews that are difficult to evaluate even for humans or that are labeled wrongly, that would be not a bad sign. \n",
        "\n",
        "It would not be bad in the sense that the model does not predict based on some arbitrary signs, but that it learned an understanding that is similiar to the human one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q55Xpla8HfT3",
        "outputId": "0cc398be-ac7c-4b89-c0f8-40dd4685a616",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "lstm_learner.view_top_losses(n=5, preproc=preproc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------\n",
            "id:3508 | loss:8.5 | true:pos | pred:neg)\n",
            "\n",
            "it's not citizen kane but it does deliver cleavage and lots of it br br badly acted and directed poorly scripted who cares i didn't watch it for the dialog\n",
            "----------\n",
            "id:10050 | loss:8.33 | true:pos | pred:neg)\n",
            "\n",
            "black tar can't be there's a documentary dark end of the street about s f street punks and b t abuse not bad quite heavy in wasted there's this stuff that looks like coke but should be something else no big deal black tar can't be there's a documentary dark end of the street about s f street punks and b t abuse not bad quite heavy in wasted there's this stuff that looks like coke but should be something else no big deal black tar can't be there's a documentary dark end of the street about s f street punks and b t abuse not bad quite heavy in wasted there's this stuff that looks like coke but should be something else no big deal\n",
            "----------\n",
            "id:1924 | loss:7.81 | true:pos | pred:neg)\n",
            "\n",
            "david morse and andre are very talented actors which is why i'm trying so hard to support this program unfortunately an irrational plot and very poor writing is making it difficult for me i'm hoping that the show gets a serious or that the actors find new projects that are worthy of them\n",
            "----------\n",
            "id:3998 | loss:7.42 | true:neg | pred:pos)\n",
            "\n",
            "this movie was pure genius john waters is brilliant it is hilarious and i am not sick of it even after seeing it about 20 times since i bought it a few months ago the acting is great although lake could have been better and johnny depp is magnificent he is such a beautiful man and a very talented actor and seeing most of johnny's movies this is probably my favorite i give it 9 5 10 rent it today\n",
            "----------\n",
            "id:7726 | loss:7.05 | true:neg | pred:pos)\n",
            "\n",
            "this is without a doubt the most hilarious movie i've ever seen seriously if the makers of this movie are ever discovered they'll put guys like jim carrey out of a job rent jack o tonight believe me you won't regret it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xhdOPIwr-X4R"
      },
      "source": [
        "**Interpretation of the errors**\n",
        "\n",
        "\n",
        "*   id 3505: The author evaluates the movie as having a bad script and bad acting, but for some reason he does not care. So, the \"true sentiment\" is in the grey area slightly towards positive as the reviewer does not really care -- he was entertained in some way. It is labeled positive but the model predicted the sentiment as negative. The model failed here, but to be fair, it is a really difficult decision.\n",
        "\n",
        "*   id 10050: This is a really interesting review. The movie is about drug addiction and, frankly, the author of the review seems to have some experience on that. He repeats one sentence 3 times and the whole review is written very messy. I googled and found out that he actually gave the movie 7 out of 10 stars. I could not have derived that based on the text. I am not alone with that: \"0 out of 2 found this helpful.\" (https://www.imdb.com/review/rw1177735/?ref_=tt_urv). I think it is no shame that the model predicted the sentiment wrongly. \n",
        "\n",
        "*   id 1924: This review is labeled incorrect. It seems pretty clear that this is a negative review. So, the model predicted the sentement correctly.\n",
        "*   id 3998: This review is mislabeled, too. The model predicted it correctly.\n",
        "*   id 7726: I can not decide myself whether the author liked the movie or found it bad in an entertaining way and ment the review ironic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fWOKy4NPIORW",
        "colab": {}
      },
      "source": [
        "lstm_predictor = ktrain.get_predictor(lstm_learner.model, preproc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F6-Ne5LMHsns",
        "colab": {}
      },
      "source": [
        "test_b = load_files(os.path.join('aclImdb',  'test'), \n",
        "                    shuffle=False, \n",
        "                    categories=['neg', 'pos'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M7UiMo27Pe_d"
      },
      "source": [
        "**Did the model learn to process contextual information?**\n",
        "\n",
        "Remember that our statistical model based on TFIDF had an accuracy of 85% on validation data. However, it struggled for document 19. I believe that is because in document 19 the meaning is heavely embedded in the context. The author does not use words that by themselves signal a strong negative sentiment, but compares an actress to an oven. The strongest words on their own are \"delightful\" and \"happy\" which falsely indicate a positive sentiment.\n",
        "\n",
        "In theory, an LSTM model is able to capture relationships by memorizing information from earlier words. We have used a bidirectional LSTM that reads the sentence backwards once and normally once and then concatenates the results. \n",
        "\n",
        "In the following we can see that the LSTM model actually classified the sentence correctly, unlike the classical, statistical model, which is based on a bag of words. That seems to indicate that the theory works. To be honest, it could also be mere luck. To really evaluate this, I would need to manually check way more examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WkLGBqp3-mbg",
        "outputId": "4dbab4c5-838b-47b2-cdaa-166276b32636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Predicted label: \"+ lstm_predictor.predict(test_b.data[19].decode('utf-8')))\n",
        "print(\"True label: \"+ str(test_b.target[19]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted label: neg\n",
            "True label: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mjZ46BmuHuYc",
        "outputId": "5fc61be3-75db-4959-8cea-f8cefa7eba9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "lstm_predictor.explain(test_b.data[19].decode('utf-8'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "    \n",
              "\n",
              "        \n",
              "\n",
              "        \n",
              "    \n",
              "        \n",
              "        \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=neg\n",
              "    \n",
              "</b>\n",
              "\n",
              "    \n",
              "    (probability <b>0.619</b>, score <b>-0.486</b>)\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
              "                    Contribution<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.707\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        &lt;BIAS&gt;\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 91.13%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.221\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        Highlighted in text (sum)\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
              "        <span style=\"background-color: hsl(0, 100.00%, 97.22%); opacity: 0.80\" title=\"-0.064\">new</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.55%); opacity: 0.82\" title=\"0.264\">york</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.37%); opacity: 0.80\" title=\"-0.030\">family</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.14%); opacity: 0.80\" title=\"-0.067\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.29%); opacity: 0.80\" title=\"-0.062\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.36%); opacity: 0.80\" title=\"-0.060\">last</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.86%); opacity: 0.81\" title=\"-0.077\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.73%); opacity: 0.81\" title=\"-0.081\">their</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.77%); opacity: 0.82\" title=\"0.304\">neighborhood</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.79%); opacity: 0.81\" title=\"-0.079\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.23%); opacity: 0.80\" title=\"-0.064\">get</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.97%); opacity: 0.80\" title=\"-0.041\">a</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.32%); opacity: 0.80\" title=\"-0.009\">television</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.58%); opacity: 0.80\" title=\"0.053\">set</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 99.83%); opacity: 0.80\" title=\"0.001\">which</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.42%); opacity: 0.80\" title=\"-0.029\">nearly</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.35%); opacity: 0.84\" title=\"0.626\">ruins</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.14%); opacity: 0.80\" title=\"-0.012\">david</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.26%); opacity: 0.81\" title=\"-0.138\">niven</span><span style=\"opacity: 0.80\">&#x27;</span><span style=\"background-color: hsl(0, 100.00%, 97.18%); opacity: 0.80\" title=\"-0.066\">s</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.72%); opacity: 0.81\" title=\"-0.207\">marriage</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.37%); opacity: 0.81\" title=\"-0.177\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.64%); opacity: 0.80\" title=\"-0.023\">mitzi</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.97%); opacity: 0.80\" title=\"0.041\">gaynor</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 88.02%); opacity: 0.84\" title=\"0.520\">bedroom</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.24%); opacity: 0.80\" title=\"-0.064\">comedy</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.08%); opacity: 0.81\" title=\"-0.146\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.00%); opacity: 0.82\" title=\"0.346\">rarely</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.85%); opacity: 0.81\" title=\"-0.201\">ventures</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.84%); opacity: 0.81\" title=\"-0.156\">into</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.36%); opacity: 0.82\" title=\"-0.326\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.83%); opacity: 0.84\" title=\"0.532\">bedroom</span><span style=\"opacity: 0.80\">(</span><span style=\"background-color: hsl(0, 100.00%, 95.37%); opacity: 0.81\" title=\"-0.134\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 75.51%); opacity: 0.90\" title=\"1.444\">nothing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.47%); opacity: 0.82\" title=\"-0.320\">sexy</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.93%); opacity: 0.81\" title=\"0.197\">happens</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.38%); opacity: 0.81\" title=\"0.094\">there</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.56%); opacity: 0.80\" title=\"-0.054\">anyway</span><span style=\"opacity: 0.80\">). </span><span style=\"background-color: hsl(120, 100.00%, 98.99%); opacity: 0.80\" title=\"0.015\">gaynor</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.15%); opacity: 0.81\" title=\"-0.187\">as</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.55%); opacity: 0.80\" title=\"0.025\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.13%); opacity: 0.85\" title=\"0.708\">actress</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.28%); opacity: 0.81\" title=\"-0.098\">has</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.27%); opacity: 0.80\" title=\"-0.033\">about</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.66%); opacity: 0.81\" title=\"-0.164\">as</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.24%); opacity: 0.83\" title=\"0.446\">much</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.38%); opacity: 0.82\" title=\"-0.325\">range</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.46%); opacity: 0.81\" title=\"-0.130\">as</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.55%); opacity: 0.80\" title=\"0.025\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.39%); opacity: 0.80\" title=\"0.007\">oven</span><span style=\"opacity: 0.80\">--</span><span style=\"background-color: hsl(0, 100.00%, 93.89%); opacity: 0.81\" title=\"-0.199\">she</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.23%); opacity: 0.81\" title=\"-0.183\">turns</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.09%); opacity: 0.80\" title=\"-0.013\">on</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 93.61%); opacity: 0.81\" title=\"-0.212\">she</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.23%); opacity: 0.81\" title=\"-0.183\">turns</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.83%); opacity: 0.81\" title=\"0.078\">off</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 97.55%); opacity: 0.80\" title=\"-0.054\">film</span><span style=\"opacity: 0.80\">&#x27;</span><span style=\"background-color: hsl(120, 100.00%, 96.33%); opacity: 0.81\" title=\"0.096\">s</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.53%); opacity: 0.85\" title=\"0.749\">sole</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.77%); opacity: 0.81\" title=\"-0.159\">compensation</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.89%); opacity: 0.82\" title=\"-0.247\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.22%); opacity: 0.81\" title=\"-0.100\">a</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.39%); opacity: 0.82\" title=\"0.222\">supporting</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.19%); opacity: 0.82\" title=\"-0.335\">performance</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.12%); opacity: 0.80\" title=\"-0.068\">by</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.56%); opacity: 0.83\" title=\"0.487\">perky</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.36%); opacity: 0.83\" title=\"-0.499\">patty</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.73%); opacity: 0.80\" title=\"-0.048\">duke</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 85.41%); opacity: 0.85\" title=\"-0.689\">pre</span><span style=\"opacity: 0.80\">-&quot;</span><span style=\"background-color: hsl(0, 100.00%, 95.16%); opacity: 0.81\" title=\"-0.143\">miracle</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.45%); opacity: 0.83\" title=\"0.376\">worker</span><span style=\"opacity: 0.80\">&quot;, </span><span style=\"background-color: hsl(0, 100.00%, 94.17%); opacity: 0.81\" title=\"-0.186\">as</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.18%); opacity: 0.81\" title=\"-0.185\">niven</span><span style=\"opacity: 0.80\">&#x27;</span><span style=\"background-color: hsl(0, 100.00%, 95.88%); opacity: 0.81\" title=\"-0.113\">s</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.15%); opacity: 0.83\" title=\"0.451\">daughter</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 90.37%); opacity: 0.83\" title=\"-0.381\">she</span><span style=\"opacity: 0.80\">&#x27;</span><span style=\"background-color: hsl(0, 100.00%, 95.17%); opacity: 0.81\" title=\"-0.142\">s</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 60.00%); opacity: 1.00\" title=\"-2.910\">delightful</span><span style=\"opacity: 0.80\">; &quot;</span><span style=\"background-color: hsl(0, 100.00%, 86.12%); opacity: 0.84\" title=\"-0.641\">happy</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.40%); opacity: 0.82\" title=\"-0.222\">anniversary</span><span style=\"opacity: 0.80\">&quot; </span><span style=\"background-color: hsl(0, 100.00%, 95.94%); opacity: 0.81\" title=\"-0.111\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.65%); opacity: 0.83\" title=\"0.422\">not</span><span style=\"opacity: 0.80\">. * </span><span style=\"background-color: hsl(0, 100.00%, 97.90%); opacity: 0.80\" title=\"-0.043\">from</span><span style=\"opacity: 0.80\"> ****</span>\n",
              "    </p>\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P5RkshrCaOpe"
      },
      "source": [
        "**Final validation on the holdout dataset**\n",
        "\n",
        "Lastly, let us validate the data based on the holdout data set. Note that I splitted the test set in a new test set and a holdout set (50/50) before training. That way, we make sure that we do not overfit towards the test data by tuning the hyperparamenter of the model.\n",
        "\n",
        "The results are quite good. We gained 5% accuracy compared to the model based on TFIDF. Furthermore, recall and precision are pretty balanced between both classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CXnO_e2yD611",
        "outputId": "d7836035-d382-40fe-f328-2d91c9c7d654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_holdout[:,1], \n",
        "                            np.around(lstm_predictor.model.predict(x_holdout)[:,1]),\n",
        "                            target_names=['neg', 'pos']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.90      0.90      0.90      6308\n",
            "         pos       0.89      0.90      0.89      6192\n",
            "\n",
            "    accuracy                           0.90     12500\n",
            "   macro avg       0.90      0.90      0.90     12500\n",
            "weighted avg       0.90      0.90      0.90     12500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SnS6IA7rotqE"
      },
      "source": [
        "One disadvantage of the model is that it requires a long time for training (about half an hour for one epoch) and that there are many hyperparameters you need to optimize.\n",
        "\n",
        "To further improve the model, we could try to add a convolution layer that \"extracts the higher-level phrase representations from the word embedding vectors\" as for example proposed by Liu and Guo (https://doi.org/10.1016/j.neucom.2019.01.078).\n",
        "\n",
        "Also, we could add a second attention layer in between both LSTM layers. I just tried it in another notebook in parallel and it slightly improved the accuracy. Unfortunatley, I have no time to rerun all my LSTM models, so I leave it like this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vZbrOWMzF51T"
      },
      "source": [
        "# 3. LSTM model with pre-trained GloVe vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GhOQV_2ZHOmu"
      },
      "source": [
        "## 3.0 Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jk5u2JaWHOp4"
      },
      "source": [
        "The benefit of using pretrained word vectors is to not start the training with zero information. Pretrained vectors already contain a structure. Similiar words are closer together and depending on the method there are other logical relationships embedded such as word analogies. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Common methods are, for example, GloVe or skip-gram. The underlying idea for training the vectors is, that words can be described by their context. \n",
        "\n",
        "\"You shall know a word by the company it keeps\" (Firth, 1957)\n",
        "\n",
        "So, the basis of GloVe vectors is a co-occurence matrix as created in the first notebook. GloVe vectors were introduced by [Pennington, Socher and Manning (2014).](https://nlp.stanford.edu/pubs/glove.pdf) They also provide several pre-trained vector sets. I used the set trained on Wikipedia with a dimension of 100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hkn4CsyCW6pQ"
      },
      "source": [
        "<img src=https://nlp.stanford.edu/projects/glove/images/man_woman.jpg width=\"500\">\n",
        "\n",
        "[Visualization of analogies of GloVe vectors](https://nlp.stanford.edu/projects/glove/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8Xc-wqaiwnVb"
      },
      "source": [
        "## 3.1 Loading GloVe vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ob9puEFdw5YH"
      },
      "source": [
        "For creation of the embedding matrix I used and slightly modified the code from this keras tutorial for the usage of pretrained vectors: https://keras.io/examples/pretrained_word_embeddings/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X-O_Is72nyc2",
        "outputId": "db3b016a-6bad-4d21-e4a8-a0f9b6a02cb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "DIR = '/content/drive/My Drive/word_vectors'\n",
        "MAX_SEQUENCE_LENGTH = 400\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "\n",
        "print('Indexing word vectors.')\n",
        "embeddings_index = {}\n",
        "with open(os.path.join(DIR, 'glove.6B.100d.txt')) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "        embeddings_index[word] = coefs\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "\n",
        "# This is the word2idex dictionary which ktrain created \n",
        "# during importing the imdb data. We now need to make the\n",
        "# connection to the imported GloVeVectors\n",
        "word_index = preproc.tok.index_word\n",
        "\n",
        "# Preparation of the embedding matrix\n",
        "print('Preparing embedding matrix.')\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for i, word in word_index.items():\n",
        "    if i >= MAX_NUM_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors.\n",
            "Found 400000 word vectors.\n",
            "Preparing embedding matrix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x_hJtlUswyqA"
      },
      "source": [
        "## 3.2 Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "snLl2B1LtPpb"
      },
      "source": [
        "### 3.2.1 Training with freezed GloVe vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YLwT7S7DGckc",
        "outputId": "69fd9952-ad6c-4e17-ad0c-cfcd14b86fe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "def load_model(input_shape, embedding_matrix, trainable):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(19999+1, # add 1 for padding token\n",
        "                        100, \n",
        "                        mask_zero=True, \n",
        "                        input_length=input_shape[0],\n",
        "                        embeddings_initializer=Constant(embedding_matrix),\n",
        "                        trainable=trainable)) \n",
        "   #model.add(SpatialDropout1D(0.4))\n",
        "    model.add(Bidirectional(LSTM(200, dropout=0.2, recurrent_dropout=0.2, \n",
        "                                 return_sequences=True)))\n",
        "    model.add(Bidirectional(LSTM(200, dropout=0.2, recurrent_dropout=0.2, \n",
        "                                 return_sequences=True)))\n",
        "    model.add(SeqWeightedAttention())\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', \n",
        "                  optimizer='adam', \n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "K.clear_session()\n",
        "input_shape = ((400),)\n",
        "glove_lstm_model = load_model(input_shape, embedding_matrix, trainable=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 400, 100)          2000000   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 400, 400)          481600    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 400, 400)          961600    \n",
            "_________________________________________________________________\n",
            "seq_weighted_attention (SeqW (None, 400)               401       \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 802       \n",
            "=================================================================\n",
            "Total params: 3,444,403\n",
            "Trainable params: 1,444,403\n",
            "Non-trainable params: 2,000,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OcjcFvLTGfS1",
        "colab": {}
      },
      "source": [
        "glove_lstm_learner = ktrain.get_learner(glove_lstm_model, \n",
        "                                        train_data=(x_train, y_train), \n",
        "                                        val_data=(x_test, y_test), \n",
        "                                        batch_size=150)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vN4uDBVMGhm9",
        "colab": {}
      },
      "source": [
        "# find a good learning rate\n",
        "#learner.lr_find()\n",
        "#learner.lr_plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1ImmeRPVGjGV",
        "outputId": "e0994760-f353-4eae-f0d4-5e758c1ab9f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "glove_lstm_learner.autofit(0.01,\n",
        "                           150, \n",
        "                           early_stopping=3, \n",
        "                           checkpoint_folder='/content/drive/My Drive/models/glove')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "begin training using triangular learning rate policy with max lr of 0.01...\n",
            "Train on 25000 samples, validate on 12500 samples\n",
            "Epoch 1/150\n",
            "25000/25000 [==============================] - 521s 21ms/sample - loss: 0.4967 - acc: 0.7475 - val_loss: 0.3308 - val_acc: 0.8571\n",
            "Epoch 2/150\n",
            "25000/25000 [==============================] - 517s 21ms/sample - loss: 0.3428 - acc: 0.8524 - val_loss: 0.2940 - val_acc: 0.8767\n",
            "Epoch 3/150\n",
            "25000/25000 [==============================] - 506s 20ms/sample - loss: 0.3016 - acc: 0.8728 - val_loss: 0.2740 - val_acc: 0.8853\n",
            "Epoch 4/150\n",
            "25000/25000 [==============================] - 500s 20ms/sample - loss: 0.2746 - acc: 0.8862 - val_loss: 0.2605 - val_acc: 0.8938\n",
            "Epoch 5/150\n",
            "25000/25000 [==============================] - 502s 20ms/sample - loss: 0.2511 - acc: 0.8986 - val_loss: 0.2609 - val_acc: 0.8917\n",
            "Epoch 6/150\n",
            "25000/25000 [==============================] - 498s 20ms/sample - loss: 0.2341 - acc: 0.9042 - val_loss: 0.2557 - val_acc: 0.8985\n",
            "Epoch 7/150\n",
            "25000/25000 [==============================] - 500s 20ms/sample - loss: 0.2192 - acc: 0.9114 - val_loss: 0.2516 - val_acc: 0.9009\n",
            "Epoch 8/150\n",
            "25000/25000 [==============================] - 505s 20ms/sample - loss: 0.1999 - acc: 0.9204 - val_loss: 0.2684 - val_acc: 0.8970\n",
            "Epoch 9/150\n",
            "25000/25000 [==============================] - 512s 20ms/sample - loss: 0.1820 - acc: 0.9278 - val_loss: 0.2636 - val_acc: 0.8972\n",
            "Epoch 10/150\n",
            "24900/25000 [============================>.] - ETA: 1s - loss: 0.1748 - acc: 0.9316Restoring model weights from the end of the best epoch.\n",
            "25000/25000 [==============================] - 503s 20ms/sample - loss: 0.1746 - acc: 0.9317 - val_loss: 0.2529 - val_acc: 0.8986\n",
            "Epoch 00010: early stopping\n",
            "Weights from best epoch have been loaded into model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f54ce78eeb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "riAm30yZjmtb",
        "colab": {}
      },
      "source": [
        "glove_lstm_learner.save_model(\n",
        "    '/content/drive/My Drive/models/glove/glove_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E8l1t627taMo"
      },
      "source": [
        "### 3.2.2 Training with unfreezed GloVe vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CQ8GzQ3XKGro"
      },
      "source": [
        "**Now** let's try the same, but allow the glove vectors to be further trained. Hopefully, this way we can finetune the vectors and archieve better results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JCBakrckKbII",
        "outputId": "e5a7cf4d-847e-45ba-a65a-5d8d6e468680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "K.clear_session()\n",
        "input_shape = ((400),)\n",
        "glove_lstm_trainable_model = load_model(input_shape, embedding_matrix, trainable=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 400, 100)          2000000   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 400, 400)          481600    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 400, 400)          961600    \n",
            "_________________________________________________________________\n",
            "seq_weighted_attention (SeqW (None, 400)               401       \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 802       \n",
            "=================================================================\n",
            "Total params: 3,444,403\n",
            "Trainable params: 3,444,403\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PNSA0RusKj_G",
        "colab": {}
      },
      "source": [
        "glove_lstm_trainable_learner = ktrain.get_learner(glove_lstm_trainable_model, \n",
        "                                        train_data=(x_train, y_train), \n",
        "                                        val_data=(x_test, y_test), \n",
        "                                        batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1gZ-oo6IKwbz",
        "outputId": "248fee91-42f3-4478-8c33-b751324934be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "glove_lstm_trainable_learner.autofit(0.01,\n",
        "                           50, \n",
        "                           early_stopping=1, \n",
        "                           checkpoint_folder='/content/drive/My Drive/models/glove')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "begin training using triangular learning rate policy with max lr of 0.01...\n",
            "Train on 25000 samples, validate on 12500 samples\n",
            "Epoch 1/50\n",
            "25000/25000 [==============================] - 1731s 69ms/sample - loss: 0.3681 - acc: 0.8265 - val_loss: 0.2451 - val_acc: 0.9002\n",
            "Epoch 2/50\n",
            "25000/25000 [==============================] - 1846s 74ms/sample - loss: 0.1662 - acc: 0.9384 - val_loss: 0.2675 - val_acc: 0.8952\n",
            "Epoch 3/50\n",
            "24960/25000 [============================>.] - ETA: 2s - loss: 0.0972 - acc: 0.9669Restoring model weights from the end of the best epoch.\n",
            "25000/25000 [==============================] - 1815s 73ms/sample - loss: 0.0972 - acc: 0.9669 - val_loss: 0.3111 - val_acc: 0.8869\n",
            "Epoch 00003: early stopping\n",
            "Weights from best epoch have been loaded into model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f502c1b4860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SIEpjez-K3o8",
        "colab": {}
      },
      "source": [
        "glove_lstm_trainable_learner.save_model(\n",
        "    '/content/drive/My Drive/models/glove/glove_trainable_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0W5OsaE7HjKZ"
      },
      "source": [
        "## 3.3 Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zPiUBUBnsSrl"
      },
      "source": [
        "### 3.3.1 Trained with unfreezed GloVe vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9B4f95Syf5XW",
        "colab": {}
      },
      "source": [
        "# I used a custom layer for attention. Unfortunately, I can not load my model\n",
        "# because of that. Luckily, I still can recreate my model and than load the\n",
        "# weights of my best epoch.\n",
        "glove_lstm_trainable_learner.model.load_weights('/content/drive/My Drive/models/glove/weights-01.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ugi9c7HErunv",
        "colab": {}
      },
      "source": [
        "glove_lstm_trainable_predictor = ktrain.get_predictor(glove_lstm_trainable_learner.model, preproc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DuqZIwcof5Xm"
      },
      "source": [
        "**Final validation on the holdout dataset**\n",
        "\n",
        "While we have an accuracy of 90% on the validation set after the best epoch (the 1st), the model performs way worse on the holdout set. I am surprised by the difference. When we look at the recall, there seems to be a bias towards the positive class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aK1hVScXf5Xm",
        "outputId": "8c929f25-3a57-4bcc-b9ac-1559a3221d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_holdout[:,1], \n",
        "                            np.around(glove_lstm_trainable_predictor.model.predict(x_holdout)[:,1]),\n",
        "                            target_names=['neg', 'pos']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.88      0.83      0.86      6308\n",
            "         pos       0.84      0.89      0.86      6192\n",
            "\n",
            "    accuracy                           0.86     12500\n",
            "   macro avg       0.86      0.86      0.86     12500\n",
            "weighted avg       0.86      0.86      0.86     12500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wl10xS7zsC3d"
      },
      "source": [
        "### 3.3.2 Trained with freezed GloVe vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gtm3X2ogsBIG",
        "colab": {}
      },
      "source": [
        "glove_lstm_learner.model.load_weights('/content/drive/My Drive/models/glove/weights-07.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gBETUExWsBIK",
        "colab": {}
      },
      "source": [
        "glove_lstm_predictor = ktrain.get_predictor(glove_lstm_learner.model, preproc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QG49gJ1CsBIM"
      },
      "source": [
        "**Final validation on the holdout dataset**\n",
        "\n",
        "The LSTM model with freezed GloVe vectors shows a stable performance on the holdout set with 90% accuracy.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TYBgt6-KsBIN",
        "outputId": "15b91a12-0740-4f5d-f3d2-9b16de92b36f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_holdout[:,1], \n",
        "                            np.around(glove_lstm_predictor.model.predict(x_holdout)[:,1]),\n",
        "                            target_names=['neg', 'pos']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.90      0.90      0.90      6308\n",
            "         pos       0.90      0.90      0.90      6192\n",
            "\n",
            "    accuracy                           0.90     12500\n",
            "   macro avg       0.90      0.90      0.90     12500\n",
            "weighted avg       0.90      0.90      0.90     12500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hEQ62b2dgsQQ"
      },
      "source": [
        "'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TtvQE9w03rtX"
      },
      "source": [
        "### 3.3.3 Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NC6VceEH3vC_"
      },
      "source": [
        "![alt text](https://)I hoped to improve the performance of the LSTM model by using pretrained GloVe vectors. However, the comparison on the holdout data shows no real improvement. The accuracy for both approaches is 90%. You could argue that the f1-score for the positive class with pretrained and freezed GloVe vectors is about 1% better, but I do not think that is significant.\n",
        "\n",
        "Using pretrained GloVe vectors and continuing to train them during the overall model training actually hurts the performance. A bias towards the positive class is introduced. Maybe the corpus is to small to effectively modify the GloVe vector in a beneficial way. Maybe we would perform better when we try to steadily defreeze.\n",
        "\n",
        "Furthermore, we could try out other GloVe vectors that are trained from other corpus or that have a higher dimension and, therefore, contain more information. Or we could try out vectors that are pretrained with other models. Intuitively, pretrained vectors based on a twitter corpus might be more beneficial as the communication on twitter is more similiar to the IMDb data set.\n",
        "\n",
        "Additionally, we can improve the architecture of the LSTM as I already suggested in the section 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yMdY6e_wJkKa"
      },
      "source": [
        "# 4. fastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UGOZJarLEtaG"
      },
      "source": [
        "## 4.0 Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0NRAaFAcFDC7"
      },
      "source": [
        "I implemented the fastText model based on the paper [Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf) (Joulin et al., 2016).\n",
        "\n",
        "This model is simple but extremely efficient. It is specially designed to give an efficient baseline for NLP classification, but the paper shows that it can often compete with deep learning models. In the first layer, the features/tokens will be embedded. In the next layer, the resulting embedded vectors will simply be averaged to from hidden variables. Lastly, there is an output layer with the number of classes and a softmax function.\n",
        "\n",
        "To get context information in the model, n-grams are used (3-grams in my implementation). Thereby, the model receives some information about the local word order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W1IX0hs9waPX"
      },
      "source": [
        "## 4.1 Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GtSOiLeWRzWo",
        "outputId": "8485033e-f5b2-49da-9391-a4b6554576b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# Importing data\n",
        "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_folder('aclImdb', \n",
        "                                                                         max_features=20000,\n",
        "                                                                         maxlen=500,\n",
        "                                                                         preprocess_mode='standard',\n",
        "                                                                         ngram_range=3,\n",
        "                                                                         classes=['pos', 'neg'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "detected encoding: utf-8\n",
            "language: en\n",
            "Word Counts: 88582\n",
            "Nrows: 25000\n",
            "25000 train sequences\n",
            "Average train sequence length: 231\n",
            "Adding 3-gram features\n",
            "max_features changed to 4678016 with addition of ngrams\n",
            "Average train sequence length with ngrams: 690\n",
            "x_train shape: (25000,500)\n",
            "y_train shape: (25000,2)\n",
            "25000 test sequences\n",
            "Average test sequence length: 224\n",
            "Average test sequence length with ngrams: 522\n",
            "x_test shape: (25000,500)\n",
            "y_test shape: (25000,2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5vKwXZnvmuyn",
        "colab": {}
      },
      "source": [
        "# Here, we split the test data to receive a holdout set. We do not want to \n",
        "# leak information by tuning hyperparameters\n",
        "x_test, x_holdout, y_test, y_holdout = train_test_split(x_test,\n",
        "                                                        y_test, \n",
        "                                                        test_size=0.5, \n",
        "                                                        random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h4Di2JiB1z7r"
      },
      "source": [
        "This time the first document looks different after preprocessing, because we created bigrams, which are represented by the indices higher than 19999."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J47CjM62YMVc",
        "colab": {}
      },
      "source": [
        "#print('feature vector of first document in train set:')\n",
        "#print(x_train[0])\n",
        "#print('target vector of first document in train set:')\n",
        "#print(y_train[0])\n",
        "#print('highest word index in train set: ' + str(np.amax(x_train)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "17uvlGEkwcld"
      },
      "source": [
        "## 4.2 Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4QmvD47cQQcg",
        "outputId": "b1f423ba-ecd9-4ca3-aa5e-c2f01db479df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "def load_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(4678015+1, 50, \n",
        "                        input_length=input_shape[0], \n",
        "                        mask_zero=True))\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', \n",
        "                  optimizer='adam', \n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "K.clear_session()\n",
        "input_shape = ((500),)\n",
        "fasttext_model= load_model(input_shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 500, 50)           233900800 \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 102       \n",
            "=================================================================\n",
            "Total params: 233,900,902\n",
            "Trainable params: 233,900,902\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9dPaRnfBQMsh",
        "colab": {}
      },
      "source": [
        "fasttext_learner = ktrain.get_learner(fasttext_model, \n",
        "                             train_data=(x_train, y_train), \n",
        "                             val_data=(x_test, y_test), \n",
        "                             batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NvxhN4qUQWH_",
        "outputId": "ab077cee-8e13-4575-b577-717cd44114ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "fasttext_learner.autofit(0.01, \n",
        "                         10, \n",
        "                         early_stopping=4, \n",
        "                         checkpoint_folder='/content/drive/My Drive/models/fasttext')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "begin training using triangular learning rate policy with max lr of 0.01...\n",
            "Train on 25000 samples, validate on 12500 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 33s 1ms/sample - loss: 0.3978 - acc: 0.8362 - val_loss: 0.2475 - val_acc: 0.9050\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 31s 1ms/sample - loss: 0.0280 - acc: 0.9920 - val_loss: 0.2574 - val_acc: 0.9043\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 33s 1ms/sample - loss: 0.0016 - acc: 0.9999 - val_loss: 0.2612 - val_acc: 0.9073\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 30s 1ms/sample - loss: 6.2980e-04 - acc: 1.0000 - val_loss: 0.2635 - val_acc: 0.9073\n",
            "Epoch 5/10\n",
            "24960/25000 [============================>.] - ETA: 0s - loss: 2.8884e-04 - acc: 1.0000Restoring model weights from the end of the best epoch.\n",
            "25000/25000 [==============================] - 28s 1ms/sample - loss: 2.8920e-04 - acc: 1.0000 - val_loss: 0.2675 - val_acc: 0.9072\n",
            "Epoch 00005: early stopping\n",
            "Weights from best epoch have been loaded into model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f19b2cf9438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pyV3oigZZIGA",
        "colab": {}
      },
      "source": [
        "fasttext_learner.save_model(\n",
        "    '/content/drive/My Drive/models/fasttext/fasttext_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9yyrnX7qHpqp"
      },
      "source": [
        "## 4.3 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g95sZPCy-kcA",
        "colab": {}
      },
      "source": [
        "fasttext_learner.load_model(\n",
        "    '/content/drive/My Drive/models/fasttext/fasttext_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qMSj61HA-kcE"
      },
      "source": [
        "**Analysing the top losses**\n",
        "\n",
        "For the same reasons as mentioned in the LSTM section, let us have a view of the 5 documents with the highest loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zrmHsrRp-kcE",
        "outputId": "ac3770fc-5f7e-4516-fae8-1ef5f310de11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "fasttext_learner.view_top_losses(n=5, preproc=preproc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------\n",
            "id:1134 | loss:16.12 | true:neg | pred:pos)\n",
            "\n",
            "masterpiece carrot top blows the screen away never has one movie captured the essence of the human spirit quite like chairman of the board 10 10 don't miss this instant classic feyder's\n",
            "----------\n",
            "id:1876 | loss:11.41 | true:neg | pred:pos)\n",
            "\n",
            "this film has the language the style and the attitude down plus greats rides from a world champ and the great jerry lopez john as turtle has the surf down and the surfing scenes are still the best ever a true classic that can be seen many times is a babe and laird hamilton shows the early stuff that has made him the world's number one extreme surfer ising\n",
            "----------\n",
            "id:3998 | loss:11.27 | true:neg | pred:pos)\n",
            "\n",
            "this movie was pure genius john waters is brilliant it is hilarious and i am not sick of it even after seeing it about 20 times since i bought it a few months ago the acting is great although lake could have been better and johnny depp is magnificent he is such a beautiful man and a very talented actor and seeing most of johnny's movies this is probably my favorite i give it 9 5 10 rent it today castmates\n",
            "----------\n",
            "id:785 | loss:11.07 | true:neg | pred:pos)\n",
            "\n",
            "this is definitely one of the best kung fu movies in the history of cinema the screenplay is really well done which is not often the case for this type of movies and you can see that chuck in one of his first role is a great actor the final fight with the deputy in the is a masterpiece politician's 'loulou's celaschi\n",
            "----------\n",
            "id:2365 | loss:10.46 | true:neg | pred:pos)\n",
            "\n",
            "this has to be one of if not the greatest mob crime films of all time every thing about this movie is great the acting in this film is of true quality master acting skills make you actually believe he is italian the cinematography is excellent too probably the best ever this movie was great and i have the brain capacity of an earth worm daunting agar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-EhvqjIy-kcH"
      },
      "source": [
        "**Interpretation of the errors**\n",
        "\n",
        "* id 1134: The review is mislabeled; fastText is correct.\n",
        "* id 1876: The review is mislabeled; fastText is correct.\n",
        "* id 3998: The review is mislabeled; fastText is correct.\n",
        "* id 789: The review is mislabeled; fastText is correct.\n",
        "* id 2365: The review is mislabeled; fastText is correct.\n",
        "\n",
        "All 5 reviews with the highest lost are actually mislabelled. It would be interesting to know the real accuracy if all labels were correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qgHTSlV9-kcK",
        "colab": {}
      },
      "source": [
        "fasttext_predictor = ktrain.get_predictor(fasttext_learner.model, preproc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XA8lOKaM-kcO"
      },
      "source": [
        "**Did the model learn to process contextual information?**\n",
        "\n",
        "Remember that our statistical model based on TFIDF had an accuracy of 85% on validation data. However, it struggled for document 19. I believe that is because in document 19 the meaning is heavely embedded in the context. The author does not use words that by themselves signal a strong negative sentiment, but compares an actress to an oven. Strong words on their own are \"delightful\" and \"happy\" which falsly indicate a positive sentiment.\n",
        "\n",
        "The fastText model does predict the sentence correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SKFSvGMh-kcQ",
        "outputId": "d1bb96c4-516d-42fb-d8e0-e99d5307ffd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Predicted label: \"+ fasttext_predictor.predict(test_b.data[19].decode('utf-8')))\n",
        "print(\"True label: \"+ str(test_b.target[19]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted label: neg\n",
            "True label: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rGOB2Wjm-kcR",
        "outputId": "8959b808-d553-4379-ed1c-3702c2abf349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "fasttext_predictor.explain(test_b.data[19].decode('utf-8'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "    \n",
              "\n",
              "        \n",
              "\n",
              "        \n",
              "    \n",
              "        \n",
              "        \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=neg\n",
              "    \n",
              "</b>\n",
              "\n",
              "    \n",
              "    (probability <b>0.879</b>, score <b>-1.985</b>)\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
              "                    Contribution<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +1.565\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        Highlighted in text (sum)\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 92.04%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.420\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        &lt;BIAS&gt;\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "\n",
              "        \n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
              "        <span style=\"background-color: hsl(120, 100.00%, 87.60%); opacity: 0.84\" title=\"0.389\">new</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.07%); opacity: 0.85\" title=\"0.507\">york</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.73%); opacity: 0.83\" title=\"-0.297\">family</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.90%); opacity: 0.82\" title=\"-0.250\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.48%); opacity: 0.85\" title=\"-0.488\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.53%); opacity: 0.83\" title=\"-0.265\">last</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.65%); opacity: 0.85\" title=\"-0.480\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.73%); opacity: 0.81\" title=\"-0.147\">their</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.60%); opacity: 0.82\" title=\"-0.186\">neighborhood</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.13%); opacity: 0.80\" title=\"-0.009\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.39%); opacity: 0.83\" title=\"-0.354\">get</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.04%); opacity: 0.81\" title=\"-0.076\">a</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.79%); opacity: 0.82\" title=\"-0.179\">television</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.47%); opacity: 0.83\" title=\"0.351\">set</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(120, 100.00%, 94.53%); opacity: 0.81\" title=\"0.121\">which</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.64%); opacity: 0.80\" title=\"0.017\">nearly</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.98%); opacity: 0.83\" title=\"0.287\">ruins</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.00%); opacity: 0.81\" title=\"0.077\">david</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.30%); opacity: 0.81\" title=\"-0.069\">niven</span><span style=\"opacity: 0.80\">&#x27;</span><span style=\"background-color: hsl(0, 100.00%, 98.33%); opacity: 0.80\" title=\"-0.022\">s</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.27%); opacity: 0.84\" title=\"0.404\">marriage</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.37%); opacity: 0.80\" title=\"-0.021\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.97%); opacity: 0.81\" title=\"-0.078\">mitzi</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.74%); opacity: 0.81\" title=\"0.058\">gaynor</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 97.55%); opacity: 0.80\" title=\"-0.038\">bedroom</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.81%); opacity: 0.81\" title=\"-0.083\">comedy</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.00%); opacity: 0.84\" title=\"0.417\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.63%); opacity: 0.83\" title=\"-0.261\">rarely</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.38%); opacity: 0.83\" title=\"-0.312\">ventures</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.49%); opacity: 0.85\" title=\"-0.487\">into</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.08%); opacity: 0.82\" title=\"-0.205\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.13%); opacity: 0.85\" title=\"0.505\">bedroom</span><span style=\"opacity: 0.80\">(</span><span style=\"background-color: hsl(120, 100.00%, 83.54%); opacity: 0.86\" title=\"0.584\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"2.075\">nothing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.22%); opacity: 0.86\" title=\"-0.652\">sexy</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 80.06%); opacity: 0.87\" title=\"0.768\">happens</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.82%); opacity: 0.83\" title=\"0.336\">there</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.62%); opacity: 0.80\" title=\"0.037\">anyway</span><span style=\"opacity: 0.80\">). </span><span style=\"background-color: hsl(120, 100.00%, 94.55%); opacity: 0.81\" title=\"0.120\">gaynor</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.60%); opacity: 0.83\" title=\"0.303\">as</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.42%); opacity: 0.81\" title=\"0.066\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.32%); opacity: 0.82\" title=\"-0.196\">actress</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.06%); opacity: 0.80\" title=\"0.050\">has</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 66.46%); opacity: 0.96\" title=\"1.613\">about</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 80.54%); opacity: 0.87\" title=\"0.741\">as</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.45%); opacity: 0.83\" title=\"0.268\">much</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.47%); opacity: 0.83\" title=\"-0.267\">range</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.90%); opacity: 0.82\" title=\"0.250\">as</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.13%); opacity: 0.84\" title=\"0.366\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.73%); opacity: 0.81\" title=\"0.058\">oven</span><span style=\"opacity: 0.80\">--</span><span style=\"background-color: hsl(120, 100.00%, 93.83%); opacity: 0.81\" title=\"0.144\">she</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.91%); opacity: 0.81\" title=\"0.080\">turns</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.12%); opacity: 0.82\" title=\"0.242\">on</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 96.11%); opacity: 0.81\" title=\"-0.074\">she</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.97%); opacity: 0.82\" title=\"-0.247\">turns</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.11%); opacity: 0.82\" title=\"0.168\">off</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 98.25%); opacity: 0.80\" title=\"-0.024\">film</span><span style=\"opacity: 0.80\">&#x27;</span><span style=\"background-color: hsl(120, 100.00%, 95.58%); opacity: 0.81\" title=\"0.089\">s</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.34%); opacity: 0.84\" title=\"0.401\">sole</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.37%); opacity: 0.83\" title=\"0.312\">compensation</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.06%); opacity: 0.86\" title=\"-0.660\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 71.99%); opacity: 0.92\" title=\"-1.247\">a</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 74.10%); opacity: 0.91\" title=\"-1.115\">supporting</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.23%); opacity: 0.86\" title=\"-0.599\">performance</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.63%); opacity: 0.80\" title=\"0.003\">by</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.08%); opacity: 0.82\" title=\"0.243\">perky</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.62%); opacity: 0.84\" title=\"-0.434\">patty</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.06%); opacity: 0.81\" title=\"0.076\">duke</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 90.01%); opacity: 0.83\" title=\"-0.286\">pre</span><span style=\"opacity: 0.80\">-&quot;</span><span style=\"background-color: hsl(0, 100.00%, 88.23%); opacity: 0.83\" title=\"-0.362\">miracle</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.29%); opacity: 0.80\" title=\"-0.006\">worker</span><span style=\"opacity: 0.80\">&quot;, </span><span style=\"background-color: hsl(0, 100.00%, 91.96%); opacity: 0.82\" title=\"-0.210\">as</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.20%); opacity: 0.81\" title=\"-0.100\">niven</span><span style=\"opacity: 0.80\">&#x27;</span><span style=\"background-color: hsl(120, 100.00%, 95.82%); opacity: 0.81\" title=\"0.082\">s</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 77.47%); opacity: 0.89\" title=\"0.913\">daughter</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 95.08%); opacity: 0.81\" title=\"-0.104\">she</span><span style=\"opacity: 0.80\">&#x27;</span><span style=\"background-color: hsl(0, 100.00%, 97.10%); opacity: 0.80\" title=\"-0.049\">s</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 77.62%); opacity: 0.89\" title=\"-0.905\">delightful</span><span style=\"opacity: 0.80\">; &quot;</span><span style=\"background-color: hsl(0, 100.00%, 83.86%); opacity: 0.85\" title=\"-0.567\">happy</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.11%); opacity: 0.82\" title=\"-0.168\">anniversary</span><span style=\"opacity: 0.80\">&quot; </span><span style=\"background-color: hsl(120, 100.00%, 86.44%); opacity: 0.84\" title=\"0.443\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 68.58%); opacity: 0.94\" title=\"1.470\">not</span><span style=\"opacity: 0.80\">. * </span><span style=\"background-color: hsl(120, 100.00%, 89.00%); opacity: 0.83\" title=\"0.328\">from</span><span style=\"opacity: 0.80\"> ****</span>\n",
              "    </p>\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TnTTIhSD-kcT"
      },
      "source": [
        "**Final validation on the holdout dataset and comparison of the speed**\n",
        "\n",
        "fastText performs about as good as the LSTM models. Yet, the time for training is a fraction of that for training an LSTM. One epoch for fastText with the batch size of 64 took 33s. In a previous set up I only created bigrams and it took me even less time (8s). An epoch with the LSTM and the same set up took 1731s (almost half an hour).\n",
        "\n",
        "Also, I spent very little time to fine tune architecture and hyperparameters of the fastText model, while I had to experiment several days with the LSTM models to get over an accuracy of 90% percent during the model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "14Fpd6tG-kcV",
        "outputId": "2999c4bb-46b7-4d45-e28d-f593029c0821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_holdout[:,1], \n",
        "                            np.around(fasttext_predictor.model.predict(x_holdout)[:,1]),\n",
        "                            target_names=['neg', 'pos']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.90      0.90      0.90      6308\n",
            "         pos       0.90      0.90      0.90      6192\n",
            "\n",
            "    accuracy                           0.90     12500\n",
            "   macro avg       0.90      0.90      0.90     12500\n",
            "weighted avg       0.90      0.90      0.90     12500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7GlMOZpE1DZI"
      },
      "source": [
        "# 5. BERT based model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xkbX4TVJHwCz"
      },
      "source": [
        "## 5.0 Introduction\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers) is a powerful technique for NLP recently [developed (2018) by Google](https://arxiv.org/abs/1810.04805v2). When it came out, it broke several records on various NLP tasks. Moreover, Google keeps the code open source and even provides a pre-trained version for download available. The model is so powerful, because it is conceptionally good and at the same time trained with a massive data set.\n",
        "\n",
        "The following picture describes a top level overview from a practitioner's point of view. As a practitioner we can profit from the freely pretrained model. For our tasks (sentiment analysis) we only need to train the classifier on top of the model.\n",
        "\n",
        "<img src=http://jalammar.github.io/images/bert-transfer-learning.png width=\"700\">\n",
        "\n",
        "[Alammar, Jay (2018). The Illustrated Transformer [Blog post]](http://jalammar.github.io/illustrated-bert/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BeKpqYUemlh5"
      },
      "source": [
        "But what makes BERT actually so powerful under the hood? We meantioned in the beginning of the notebook that LSTM have a limitation. They can only read in the data from one side. Therefore, we implemented a bidirectional LSTM that concatenates output vectors from two LSTMs that read the data in different directions. The language model ELMo uses such a kind of achitecture to create bidirectional word vectors. But that is only a suboptimal solution as it is a shallow bidirectional representation compared to BERT's approach (the vectors are only a concatenation of two unidirectional trained vectors).\n",
        "\n",
        "BERT's key feature is that it is designed to create a bidirectional representation by \"jointly conditioning on both left and right context **in all layers**\" (Devlin et al., 2018). To archieve this Google uses a new technique for pre-training: they mask out some words in the middle of the sentence and then condition each word bidirectionally to predict the masked words. \n",
        "\n",
        "<img src=https://2.bp.blogspot.com/-pNxcHHXNZg0/W9iv3evVyOI/AAAAAAAADfA/KTSvKXNzzL0W8ry28PPl7nYI1CG_5WuvwCLcBGAs/s1600/f1.png width=\"700\">\n",
        "\n",
        "[Google AI Blog. Open Sourcing BERT (2018)](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FNKO0J8EGfWv"
      },
      "source": [
        "## 5.1 Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RffTn-75gqYs",
        "outputId": "cf06a343-6bb7-420b-cd69-f43cefca831e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_folder('aclImdb', \n",
        "                                                                         maxlen=400, \n",
        "                                                                         max_features = 20000,\n",
        "                                                                         preprocess_mode='bert',\n",
        "                                                                         classes=['pos', 'neg'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "detected encoding: utf-8\n",
            "downloading pretrained BERT model (uncased_L-12_H-768_A-12.zip)...\n",
            "[]\n",
            "extracting pretrained BERT model...\n",
            "done.\n",
            "\n",
            "cleanup downloaded zip...\n",
            "done.\n",
            "\n",
            "preprocessing train...\n",
            "language: en\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "done."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "preprocessing test...\n",
            "language: en\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "done."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ak7B0Y4vdNqt",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "\n",
        "# Shuffling x_test \n",
        "x_test[0], x_test[1], y_test = shuffle(x_test[0], x_test[1], y_test)\n",
        "\n",
        "# Creating a holdout set for a final validation\n",
        "x_holdout = [x_test[0][:12500], x_test[1][:12500]]\n",
        "y_holdout = y_test[:12500]\n",
        "x_test = [x_test[0][12500:], x_test[1][12500:]]\n",
        "y_test = y_test[12500:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G8bVwgLmG0Ar"
      },
      "source": [
        "## 5.2 Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4IED60Zfh_d1",
        "outputId": "f0d50006-b763-409e-8e2f-5aa64b029bbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "bert_model = text.text_classifier('bert', (x_train, y_train), preproc=preproc)\n",
        "bert_learner = ktrain.get_learner(bert_model, \n",
        "                                  train_data=(x_train, y_train), \n",
        "                                  val_data=(x_test, y_test),\n",
        "                                  batch_size=6)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is Multi-Label? False\n",
            "maxlen is 400\n",
            "done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iTx0RwTujVlI",
        "outputId": "4f68f20c-c349-498b-fd6c-e5dbfebb8512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "bert_learner.autofit(2e-5, \n",
        "                     10, \n",
        "                     early_stopping=2, \n",
        "                     checkpoint_folder='/content/drive/My Drive/models/bert')\n",
        "\n",
        "# epoch1: 9348"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "begin training using triangular learning rate policy with max lr of 2e-05...\n",
            "Train on 25000 samples, validate on 12500 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 1867s 75ms/sample - loss: 0.2527 - acc: 0.8939 - val_loss: 0.1660 - val_acc: 0.9350\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 1839s 74ms/sample - loss: 0.1363 - acc: 0.9515 - val_loss: 0.1656 - val_acc: 0.9375\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 1837s 73ms/sample - loss: 0.0778 - acc: 0.9750 - val_loss: 0.1898 - val_acc: 0.9354\n",
            "Epoch 4/10\n",
            "24996/25000 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9848Restoring model weights from the end of the best epoch.\n",
            "25000/25000 [==============================] - 1844s 74ms/sample - loss: 0.0488 - acc: 0.9848 - val_loss: 0.2056 - val_acc: 0.9354\n",
            "Epoch 00004: early stopping\n",
            "Weights from best epoch have been loaded into model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f29cfdff7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SF73GfXEkjHM",
        "colab": {}
      },
      "source": [
        "bert_learner.save_model(\n",
        "    '/content/drive/My Drive/models/bert/bert_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QAek-aLNH0wB"
      },
      "source": [
        "## 5.3 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IXzXEcaeHq2S",
        "colab": {}
      },
      "source": [
        "bert_learner.load_model(\n",
        "    '/content/drive/My Drive/models/bert/bert_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cV5ylbSYHq2X"
      },
      "source": [
        "**Analysing the top losses**\n",
        "\n",
        "Fore the same reasons as mentioned in the LSTM section, let us have a view of the 5 documents with the highest loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "54VaemjZHq2X",
        "outputId": "93c94efa-633b-434e-df56-a160b7191399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "bert_learner.view_top_losses(n=5, preproc=preproc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------\n",
            "id:11635 | loss:6.51 | true:neg | pred:pos)\n",
            "\n",
            "[CLS] i really liked this qui ##rky movie . the characters are not the bland beautiful people that show up in so many movies and on tv . it has a realistic edge , with a capt ##ivating story line . the main title sequence alone makes this movie fun to watch . [SEP]\n",
            "----------\n",
            "id:11824 | loss:6.45 | true:neg | pred:pos)\n",
            "\n",
            "[CLS] this has to be one of , if not the greatest mob / crime films of all time . every thing about this movie is great , the acting in this film is of true quality ; master p ' s acting skills make you actually believe he is italian ! the cinematography is excellent too , probably the best ever . this movie was great ; and i have the brain capacity of an earth worm . [SEP]\n",
            "----------\n",
            "id:7873 | loss:6.39 | true:pos | pred:neg)\n",
            "\n",
            "[CLS] this was one of the worst col ##umb ##o episodes that i have seen , however , i am only in the second season . < br / > < br / > the typical col ##umb ##o activities are both amusing and irritating . his cigar ashes causing him trouble have been seen before , and the bit where he always identifies in some way with the murderer - - in this case cooking , tho the scene on the tv cooking show distracted from the main theme . < br / > < br / > also not explained was why the brother at the beginning of the show was cutting part of the wires of the mixer . the reason was never explained , nor did it serve any purpose . but the part i disliked the most was the death of the bride to be . this was never explained and it is the main reason why i give this episode such a low grade . [SEP]\n",
            "----------\n",
            "id:3114 | loss:6.37 | true:neg | pred:pos)\n",
            "\n",
            "[CLS] i went to see this 3 nights ago here in cork , ireland . it was the world premiere of it , in the tiny cinema in the tri ##ske ##l arts centre as part of the cork film festival . < br / > < br / > i found \" strange fruit \" to be an excellent movie . it is a bit rough around the edges , but for a low - budget movie that is to be expected ! in general the acting ( particularly from the main lead kent fa ##ul ##con ) is wonderful , the cinematography and direction excellent , and the script hugely entertaining and thought - pro ##voking , with some nice set - ups and witty dialogue . < br / > < br / > the ending was a bit sudden , with no conclusion given to characters and events once the finale came to its gripping end . . . but perhaps that ' s what the filmmakers were going for ? it certainly did make the movie more un ##sett ##ling . i did like the fact that the main character never came to terms with his mother on screen : it leaves you wondering whether or not he ever will , as in real - life sometimes these things are never settled . this was a good choice , to leave it un ##res ##olved rather than sentimental ##ly wrapping it up ! < br / > < br / > taut and suspense ##ful throughout , \" strange fruit \" is a hugely ambitious debut and i have high hopes for what the writer / director kyle sc ##hic ##k ##ner will un ##lea ##sh next . he - and his colleagues - are a talent worth watching . < br / > < br / > i hope \" strange fruit \" gets a wider release soon , as more people deserve to see this movie , an above - average thriller with some original and insight ##ful twists on homo ##phobia and racism in america ' s deep south . < br / > < br / > highly recommended : 7 / 10 [SEP]\n",
            "----------\n",
            "id:4324 | loss:6.33 | true:neg | pred:pos)\n",
            "\n",
            "[CLS] this movie was pure genius . john waters is brilliant . it is hilarious and i am not sick of it even after seeing it about 20 times since i bought it a few months ago . the acting is great , although rick ##i lake could have been better . and johnny de ##pp is magnificent . he is such a beautiful man and a very talented actor . and seeing most of johnny ' s movies , this is probably my favorite . i give it 9 . 5 / 10 . rent it today ! [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p5J6qmDUHq2a"
      },
      "source": [
        "**Interpretation of the errors**\n",
        "\n",
        "* id 11635: The review is mislabeled; BERT is correct.\n",
        "* id 11824: The review is mislabeled; BERT is correct.\n",
        "* id 7873: The review is mislabeled; BERT is correct.\n",
        "* id 3114: The review is mislabeled; BERT is correct.\n",
        "* id 3114: The review is mislabeled; BERT is correct.\n",
        "\n",
        "All the highest losses are caused by misclassified data. Actually, there is not even any ambiguity in how to interpret these reviews, which makes sense as the BERT model is working quite good and the highest losses result by a confident predicten in the \"wrong\" direction. We might could cut off a certain percent of the list of highest losses to identify wrongly labeled data. Then we could measure the true accuracy more accurate. But, of course, that would bare a huge risk in throwing out true wrongly predicted documents which would inflate the accuracy rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yrvJrKvUHq2a",
        "colab": {}
      },
      "source": [
        "bert_predictor = ktrain.get_predictor(bert_learner.model, preproc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aB3KOyAXHq2c"
      },
      "source": [
        "**Did the model learn to process contextual information?**\n",
        "\n",
        "Remember that our statistical model based on TFIDF had an accuracy of 85% on validation data. However, it struggled for document 19. I believe that is because in document 19 the meaning is heavely embedded in the context. The author does not use words that by themselves signal a strong negative sentiment, but compares an actress to an oven. Strong words on their own are \"delightful\" and \"happy\" which falsly indicate a positive sentiment.\n",
        "\n",
        "However, the BERT model does predict the sentence correctly and it does that with the highest confidents (0.926) of all models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qQOH4hckHq2d",
        "outputId": "aea62faf-d45e-467f-cb09-d80db103e172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Predicted label: \"+ bert_predictor.predict(test_b.data[19].decode('utf-8')))\n",
        "print(\"True label: \"+ str(test_b.target[19]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted label: neg\n",
            "True label: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6fXjLMsbHq2g",
        "outputId": "5bbdfad9-d7b7-489d-ffc6-7c748e7e01df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "bert_predictor.explain(test_b.data[19].decode('utf-8'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "    \n",
              "\n",
              "        \n",
              "\n",
              "        \n",
              "    \n",
              "        \n",
              "        \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=neg\n",
              "    \n",
              "</b>\n",
              "\n",
              "    \n",
              "    (probability <b>0.926</b>, score <b>-2.531</b>)\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
              "                    Contribution<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +2.192\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        Highlighted in text (sum)\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 94.59%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.339\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        &lt;BIAS&gt;\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "\n",
              "        \n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
              "        <span style=\"background-color: hsl(120, 100.00%, 97.48%); opacity: 0.80\" title=\"0.031\">new</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.31%); opacity: 0.80\" title=\"-0.034\">york</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.14%); opacity: 0.80\" title=\"0.038\">family</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.51%); opacity: 0.83\" title=\"-0.208\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.73%); opacity: 0.82\" title=\"-0.142\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.03%); opacity: 0.81\" title=\"0.060\">last</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.23%); opacity: 0.80\" title=\"0.036\">in</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.92%); opacity: 0.81\" title=\"-0.085\">their</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.00%); opacity: 0.81\" title=\"0.083\">neighborhood</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.85%); opacity: 0.81\" title=\"-0.087\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.52%); opacity: 0.80\" title=\"-0.015\">get</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.77%); opacity: 0.80\" title=\"-0.011\">a</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.82%); opacity: 0.81\" title=\"0.044\">television</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.57%); opacity: 0.81\" title=\"0.048\">set</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 91.60%); opacity: 0.82\" title=\"-0.175\">which</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.48%); opacity: 0.86\" title=\"-0.499\">nearly</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.01%); opacity: 0.80\" title=\"0.008\">ruins</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.81%); opacity: 0.80\" title=\"0.011\">david</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.32%); opacity: 0.81\" title=\"-0.054\">niven</span><span style=\"opacity: 0.80\">&#x27;</span><span style=\"background-color: hsl(0, 100.00%, 98.29%); opacity: 0.80\" title=\"-0.018\">s</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.05%); opacity: 0.82\" title=\"-0.191\">marriage</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.63%); opacity: 0.81\" title=\"-0.047\">to</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.27%); opacity: 0.82\" title=\"-0.127\">mitzi</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.39%); opacity: 0.80\" title=\"-0.033\">gaynor</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 94.43%); opacity: 0.81\" title=\"0.097\">bedroom</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.47%); opacity: 0.86\" title=\"-0.459\">comedy</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.65%); opacity: 0.86\" title=\"-0.452\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.98%); opacity: 0.80\" title=\"0.040\">rarely</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.77%); opacity: 0.81\" title=\"-0.066\">ventures</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.09%); opacity: 0.82\" title=\"-0.160\">into</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.69%); opacity: 0.81\" title=\"-0.116\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.64%); opacity: 0.81\" title=\"-0.092\">bedroom</span><span style=\"opacity: 0.80\">(</span><span style=\"background-color: hsl(120, 100.00%, 85.44%); opacity: 0.85\" title=\"0.383\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 61.43%); opacity: 0.99\" title=\"1.540\">nothing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 80.01%); opacity: 0.87\" title=\"0.603\">sexy</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.15%); opacity: 0.82\" title=\"-0.158\">happens</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.46%); opacity: 0.80\" title=\"-0.003\">there</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.16%); opacity: 0.80\" title=\"-0.037\">anyway</span><span style=\"opacity: 0.80\">). </span><span style=\"background-color: hsl(0, 100.00%, 95.57%); opacity: 0.81\" title=\"-0.070\">gaynor</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.83%); opacity: 0.81\" title=\"0.064\">as</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.15%); opacity: 0.81\" title=\"0.057\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.83%); opacity: 0.82\" title=\"0.198\">actress</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.24%); opacity: 0.80\" title=\"-0.036\">has</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.02%); opacity: 0.82\" title=\"0.162\">about</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.83%); opacity: 0.81\" title=\"0.064\">as</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.96%); opacity: 0.81\" title=\"0.109\">much</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.02%); opacity: 0.83\" title=\"-0.256\">range</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.48%); opacity: 0.81\" title=\"-0.122\">as</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.15%); opacity: 0.81\" title=\"0.057\">an</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.09%); opacity: 0.82\" title=\"0.132\">oven</span><span style=\"opacity: 0.80\">--</span><span style=\"background-color: hsl(0, 100.00%, 87.74%); opacity: 0.84\" title=\"-0.300\">she</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.00%); opacity: 0.81\" title=\"-0.108\">turns</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.55%); opacity: 0.81\" title=\"-0.094\">on</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 92.90%); opacity: 0.82\" title=\"-0.137\">she</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.57%); opacity: 0.80\" title=\"-0.014\">turns</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.90%); opacity: 0.81\" title=\"0.086\">off</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(120, 100.00%, 81.08%); opacity: 0.87\" title=\"0.557\">film</span><span style=\"opacity: 0.80\">&#x27;</span><span style=\"background-color: hsl(120, 100.00%, 76.47%); opacity: 0.89\" title=\"0.761\">s</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 82.26%); opacity: 0.86\" title=\"0.508\">sole</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.28%); opacity: 0.85\" title=\"0.428\">compensation</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.32%); opacity: 0.81\" title=\"0.100\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.20%); opacity: 0.82\" title=\"-0.129\">a</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.99%); opacity: 0.80\" title=\"-0.040\">supporting</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.28%); opacity: 0.81\" title=\"0.077\">performance</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.56%); opacity: 0.81\" title=\"0.070\">by</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.27%); opacity: 0.81\" title=\"0.101\">perky</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.28%); opacity: 0.82\" title=\"0.155\">patty</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.40%); opacity: 0.81\" title=\"0.052\">duke</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 95.19%); opacity: 0.81\" title=\"-0.079\">pre</span><span style=\"opacity: 0.80\">-&quot;</span><span style=\"background-color: hsl(0, 100.00%, 98.23%); opacity: 0.80\" title=\"-0.019\">miracle</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.36%); opacity: 0.81\" title=\"-0.075\">worker</span><span style=\"opacity: 0.80\">&quot;, </span><span style=\"background-color: hsl(0, 100.00%, 99.02%); opacity: 0.80\" title=\"-0.008\">as</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.32%); opacity: 0.81\" title=\"-0.054\">niven</span><span style=\"opacity: 0.80\">&#x27;</span><span style=\"background-color: hsl(0, 100.00%, 95.77%); opacity: 0.81\" title=\"-0.066\">s</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.21%); opacity: 0.81\" title=\"0.056\">daughter</span><span style=\"opacity: 0.80\">. </span><span style=\"background-color: hsl(0, 100.00%, 91.36%); opacity: 0.82\" title=\"-0.182\">she</span><span style=\"opacity: 0.80\">&#x27;</span><span style=\"background-color: hsl(0, 100.00%, 79.38%); opacity: 0.88\" title=\"-0.630\">s</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 60.00%); opacity: 1.00\" title=\"-1.623\">delightful</span><span style=\"opacity: 0.80\">; &quot;</span><span style=\"background-color: hsl(0, 100.00%, 88.94%); opacity: 0.83\" title=\"-0.259\">happy</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.18%); opacity: 0.81\" title=\"-0.057\">anniversary</span><span style=\"opacity: 0.80\">&quot; </span><span style=\"background-color: hsl(120, 100.00%, 97.29%); opacity: 0.80\" title=\"0.035\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 74.81%); opacity: 0.90\" title=\"0.838\">not</span><span style=\"opacity: 0.80\">. * from ****</span>\n",
              "    </p>\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ps4jvsHXHq2i"
      },
      "source": [
        "**Final validation on the holdout dataset and comparison of the speed**\n",
        "\n",
        "Clearly, the BERT model performs best. The accuracy is 4% higher than accuracy of the LSTM and the fastText model and 9% higher than the accuracy of the random forest based on TFIDF. Considering the recall, there is a small bias towards the positive class. \n",
        "\n",
        "For comparison, the to date state of the art (21.12.19) for sentiment analysis on the IMDb dataset is 97.4% and the first time that 94% accuracy was archieved was in 2016 with an oh-LSTM: https://paperswithcode.com/sota/sentiment-analysis-on-imdb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VZAtjH92Hq2j",
        "outputId": "62bd63d0-8ec9-43bd-b370-3df07fe46859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_holdout[:,1], \n",
        "                            np.around(bert_predictor.model.predict(x_holdout)[:,1]),\n",
        "                            target_names=['neg', 'pos']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.95      0.92      0.94      6263\n",
            "         pos       0.93      0.95      0.94      6237\n",
            "\n",
            "    accuracy                           0.94     12500\n",
            "   macro avg       0.94      0.94      0.94     12500\n",
            "weighted avg       0.94      0.94      0.94     12500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HvC4tGlij8-l",
        "outputId": "85453ee9-94c3-473e-a0d1-2ef189cf75d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = [ 'Sesame street sucks!', 'I like Elmo.']\n",
        "bert_predictor.predict(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['neg', 'pos']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    }
  ]
}